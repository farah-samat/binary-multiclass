{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Libraries & Modules</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part 1: Breast Cancer Diagnosis</u>\n",
    "#### 1.1 Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 datapoints in training set and 31 columns which include label and features.\n",
      "\n",
      "Check first row (datapoint) in training set:\n",
      "[['B' 14.02 15.66 89.59 606.5 0.07966000000000001 0.055810000000000005\n",
      "  0.02087 0.02652 0.1589 0.05586 0.2142 0.6549 1.6059999999999999 19.25\n",
      "  0.004837 0.009238 0.009212999999999999 0.010759999999999999 0.01171\n",
      "  0.002104 14.91 19.31 96.53 688.9 0.1034 0.1017 0.0626 0.08216 0.2136\n",
      "  0.0671]]\n",
      "\n",
      "Check first row (datapoint) in test set:\n",
      "[['B' 8.597000000000001 18.6 54.09 221.2 0.1074 0.05847 0.0 0.0 0.2163\n",
      "  0.07359 0.3368 2.7769999999999997 2.222 17.81 0.02075 0.01403 0.0 0.0\n",
      "  0.06146 0.0068200000000000005 8.952 22.44 56.65 240.1 0.1347 0.07767 0.0\n",
      "  0.0 0.3142 0.08116]]\n",
      "\n",
      "The first element is the label, therefore:\n",
      "No. of features in training data is 30\n",
      "No. of features in test data is 30\n"
     ]
    }
   ],
   "source": [
    "train_df = (pd.read_csv(\"data/wisconsin_data/train_wbcd.csv\")).drop('Patient_ID', axis=1)\n",
    "train = train_df.values\n",
    "\n",
    "test_df = (pd.read_csv(\"data/wisconsin_data/test_wbcd.csv\")).drop('Patient_ID', axis=1)\n",
    "test = test_df.values\n",
    "\n",
    "print 'There are {} datapoints in training set and {} columns which include label and features.'.format(train.shape[0],train.shape[1])\n",
    "print \n",
    "print 'Check first row (datapoint) in training set:'\n",
    "print train[:1]\n",
    "print \n",
    "print 'Check first row (datapoint) in test set:'\n",
    "print test[:1]\n",
    "print \n",
    "print 'The first element is the label, therefore:'\n",
    "print \"No. of features in training data is {}\".format(train.shape[1] - 1)\n",
    "print \"No. of features in test data is {}\".format(test.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFrequency\n",
      "\t\t  B (0)\t\t  M (1)\n",
      "Training set\t58.000000\t42.000000\n",
      "Test set\t14.000000\t6.000000\n"
     ]
    }
   ],
   "source": [
    "print \"\\t\\t\\tFrequency\"\n",
    "print \"\\t\\t  B (0)\\t\\t  M (1)\"\n",
    "print \"Training set\\t{:f}\\t{:f}\".format((train == 'B').sum(),(train == 'M').sum())\n",
    "print \"Test set\\t{:f}\\t{:f}\".format((test == 'B').sum(),(test == 'M').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution on training and test sets shows that there are more samples with B cancer diagnosis. This is an imbalanced class problem since the classes (B and M) are not represented equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values in data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Feature 21 has 2 missing value(s)\n",
      "Test Data: Feature 21 has 1 missing value(s)\n"
     ]
    }
   ],
   "source": [
    "def na_count(df, dataset):\n",
    "    counter = 0\n",
    "    for i in range(len(df.columns)):\n",
    "        na_count = df.iloc[:,i].isnull().sum()\n",
    "        if na_count >= 1:\n",
    "            print '{} Feature {} has {} missing value(s)'.format(dataset, df.columns[i],str(na_count))\n",
    "            counter += 1\n",
    "    if counter == 0:\n",
    "        print 'No column has NA in {}'.format(dataset)\n",
    "        \n",
    "na_count(pd.DataFrame(train), \"Training Data:\")\n",
    "na_count(pd.DataFrame(test), \"Test Data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data distribution of feature 21:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAE/CAYAAAA39zBmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e5f0690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax[0].hist(train_df.iloc[:,21].dropna())\n",
    "ax[0].set_title('Data distribution of feature 21 in training dataset')\n",
    "\n",
    "ax[1].hist(test_df.iloc[:,21].dropna())\n",
    "ax[1].set_title('Data distribution of feature 21 in test dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distributions are positively skewed, the best option to replace the missing entry is using the median as mean will be influenced by the distribution tail (high values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No column has NA in Training Data\n",
      "No column has NA in Test Data\n"
     ]
    }
   ],
   "source": [
    "train_df.iloc[:,21] = train_df.iloc[:,21].fillna(train_df.iloc[:,21].median())\n",
    "test_df.iloc[:,21] = test_df.iloc[:,21].fillna(test_df.iloc[:,21].median())\n",
    "na_count(train_df, \"Training Data\")\n",
    "na_count(test_df, \"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(train_df.iloc[0:,2:])\n",
    "\n",
    "def scaler(df, columns):    \n",
    "        std_scale = preprocessing.StandardScaler().fit(df[columns])\n",
    "        df_std_mat = std_scale.transform(df[columns])\n",
    "        df_std = pd.DataFrame(df_std_mat, columns = columns)\n",
    "        df_std['Diagnosis'] = df['Diagnosis']\n",
    "        return df_std\n",
    "    \n",
    "trainNorm = scaler(train_df, features)\n",
    "testNorm = scaler(test_df, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Create the training/testing data and labels\n",
    "trainNorm = trainNorm.replace(['B', 'M'], [0, 1]) \n",
    "Xtrain = trainNorm.iloc[0:,0:29]\n",
    "ytrain = trainNorm.iloc[0:,29:]\n",
    "\n",
    "testNorm = testNorm.replace(['B', 'M'], [0, 1]) \n",
    "Xtest = testNorm.iloc[0:,0:29]\n",
    "ytest = testNorm.iloc[0:,29:]\n",
    "\n",
    "print type(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using L1 regularization, alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\t\tScore\n",
      "Recall\t\t0.833333\n",
      "Precision\t0.833333\n",
      "Accuracy\t0.900000\n",
      "F1 score\t0.833333\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "AUC:\n",
      "0.880952380952\n"
     ]
    }
   ],
   "source": [
    "# initialize the logisitc regression model. \n",
    "logistic1 = LogisticRegression(C=10, penalty='l1')\n",
    "\n",
    "# fit model\n",
    "l1 = logistic1.fit(Xtrain, ytrain)\n",
    "\n",
    "pred_label = l1.predict(Xtest)\n",
    "true_label = ytest\n",
    "\n",
    "\n",
    "# metrics for test set\n",
    "test_recall = metrics.recall_score(true_label, pred_label)\n",
    "test_precision = metrics.precision_score(true_label, pred_label)\n",
    "test_accuracy = metrics.accuracy_score(true_label, pred_label)\n",
    "test_f_score = metrics.f1_score(true_label, pred_label)\n",
    "\n",
    "\n",
    "print \"Metrics\\t\\tScore\"\n",
    "print \"Recall\\t\\t{:f}\".format(test_recall)\n",
    "print \"Precision\\t{:f}\".format(test_precision)\n",
    "print \"Accuracy\\t{:f}\".format(test_accuracy)\n",
    "print \"F1 score\\t{:f}\".format(test_f_score)\n",
    "\n",
    "print \n",
    "print 'Confusion matrix:'\n",
    "print metrics.confusion_matrix(true_label, pred_label)\n",
    "print 'AUC:'\n",
    "print roc_auc_score(true_label, pred_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using L2 regularization, lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:0.833333333333\n",
      "Precision:0.833333333333\n",
      "Accuracy:0.9\n",
      "F1 score:0.833333333333\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "AUC:\n",
      "0.880952380952\n"
     ]
    }
   ],
   "source": [
    "# initialize the logisitc regression model. \n",
    "logistic2 = LogisticRegression(C=10, penalty='l2')\n",
    "\n",
    "# fit model\n",
    "l2 = logistic2.fit(Xtrain, ytrain)\n",
    "\n",
    "pred_label = l2.predict(Xtest)\n",
    "true_label = ytest\n",
    "\n",
    "#model evaluation\n",
    "recall = metrics.recall_score(true_label, pred_label)\n",
    "precision = metrics.precision_score(true_label, pred_label)\n",
    "accuracy = metrics.accuracy_score(true_label, pred_label)\n",
    "f_score = metrics.f1_score(true_label, pred_label)\n",
    "\n",
    "print 'Recall:{}'.format(recall)\n",
    "print 'Precision:{}'.format(precision)\n",
    "print 'Accuracy:{}'.format(accuracy)\n",
    "print 'F1 score:{}'.format(f_score)\n",
    "print \n",
    "print 'Confusion matrix:'\n",
    "print metrics.confusion_matrix(true_label, pred_label)\n",
    "print 'AUC:'\n",
    "print roc_auc_score(true_label, pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Choosing the best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- L1<br>Choosing the best alpha value from the following set: {0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha values\n",
    "alpha = [0.1,1,3, 10,33,100,333,1000, 3333, 10000, 33333]\n",
    "\n",
    "#data\n",
    "trainNorm = scaler(train_df, features)\n",
    "trainNorm = trainNorm.replace(['B', 'M'], [0, 1]) \n",
    "\n",
    "Xtrain = trainNorm[features]\n",
    "ytrain = trainNorm['Diagnosis']\n",
    "\n",
    "testNorm = scaler(test_df, features)\n",
    "testNorm = testNorm.replace(['B', 'M'], [0, 1]) \n",
    "\n",
    "Xtest = testNorm[features]\n",
    "ytest = testNorm['Diagnosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function\n",
    "def runLRmodel(trials, data, predictors, label, penalty_type, penalty_score):\n",
    "    \n",
    "    model_acc     = 0\n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        Dtrain, Dtest = train_test_split(data, test_size=0.3)\n",
    "        lr = LogisticRegression(C=1/penalty_score, penalty=penalty_type)\n",
    "        lr.fit(Dtrain[predictors], Dtrain[label])\n",
    "        y_predict = lr.predict(Dtest[predictors])\n",
    "        model_acc += accuracy_score(y_predict, Dtest[label])\n",
    "        \n",
    "    model_acc /= trials\n",
    "    \n",
    "    return np.round(model_acc, decimals=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best alpha value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l1_acc = np.zeros(len(alpha))\n",
    "index = 0\n",
    "\n",
    "#L1 regularization\n",
    "for l in alpha:\n",
    "    l1_acc[index] = runLRmodel(100, trainNorm, features, 'Diagnosis', 'l1', float(l))\n",
    "    index += 1\n",
    "    \n",
    "# penalty at which validation accuracy is maximum\n",
    "max_index_l1  = np.argmax(l1_acc)\n",
    "best_alpha = alpha[max_index_l1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best alpha value to refit the model and find top 5 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]\n",
      "Acc: [ 0.99  0.99  0.97  0.95  0.59  0.56  0.57  0.58  0.58  0.57  0.58]\n",
      "Best Alpha: 0.1\n",
      "\n",
      "Precision:0.833333333333\n",
      "Accuracy:0.9\n",
      "\n",
      "Top 5 features in decreasing order of feature weights:\n",
      "1  Weight, Feature('6.45', 'f23')\n",
      "2  Weight, Feature('3.93', 'f22')\n",
      "3  Weight, Feature('2.44', 'f21')\n",
      "4  Weight, Feature('2.42', 'f25')\n",
      "5  Weight, Feature('1.5', 'f4')\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "AUC:\n",
      "0.880952380952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Alpha: {}\".format(alpha)\n",
    "print \"Acc: {}\".format(l1_acc)\n",
    "print \"Best Alpha: {}\".format(best_alpha)\n",
    "print \n",
    "\n",
    "l1 = LogisticRegression(C=1/best_alpha, penalty='l1')\n",
    "l1.fit(Xtrain, ytrain)\n",
    "pred_label = l1.predict(Xtest)\n",
    "true_label = ytest\n",
    "\n",
    "#model evaluation\n",
    "recall = recall_score(true_label, pred_label)\n",
    "precision = precision_score(true_label, pred_label)\n",
    "accuracy = accuracy_score(true_label, pred_label)\n",
    "f_score = f1_score(true_label, pred_label)\n",
    "\n",
    "print 'Precision:{}'.format(precision)\n",
    "print 'Accuracy:{}'.format(accuracy)\n",
    "\n",
    "def top_5_feature(model):\n",
    "    coeff = abs(np.round(model.coef_[0,], decimals=2))\n",
    "    coeff_dict = dict()\n",
    "    index = 0\n",
    "    for i in features[0:30]:\n",
    "        coeff_dict[i] = str(list(coeff)[index])\n",
    "        index += 1\n",
    "    top_5 = sorted([(v,k) for k,v in coeff_dict.items()], reverse=True)[0:5]\n",
    "    rank = 1\n",
    "    for i in top_5:\n",
    "        print '{} '.format(str(rank)), 'Weight, Feature{}'.format(i)\n",
    "        rank += 1\n",
    "\n",
    "        \n",
    "\n",
    "print 'Top 5 features in decreasing order of feature weights:'\n",
    "top_5_feature(l1)\n",
    "print \n",
    "\n",
    "print 'Confusion matrix:'\n",
    "print metrics.confusion_matrix(pred_label, true_label)\n",
    "print 'AUC:'\n",
    "print roc_auc_score(true_label, pred_label)\n",
    "print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion for regularized model with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model drops signficantly from alpha = 33 to alpha = 3333. Higher alpha values produce more regularized models and hence underfitting. Therefore, it can be said that the model will be underfitted if the alpha value is large.\n",
    "\n",
    "Choosing alpha value = 0.1 might result overfitted model since the larger the C value (from small alpha value), the less regularized the model.\n",
    "\n",
    "Since alpha value = 1 gives the same model performance as alpha = 0.1, it is probably the better choice to avoid overfitted models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2<br>Choosing the best alpha value from the following set: {0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lambda values\n",
    "lambda_val = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3,  1, 3, 10, 33]\n",
    "\n",
    "#data\n",
    "trainNorm = scaler(train_df, features)\n",
    "trainNorm = trainNorm.replace(['B', 'M'], [0, 1]) \n",
    "\n",
    "Xtrain = trainNorm[features]\n",
    "ytrain = trainNorm['Diagnosis']\n",
    "\n",
    "testNorm = scaler(test_df, features)\n",
    "testNorm = testNorm.replace(['B', 'M'], [0, 1]) \n",
    "\n",
    "Xtest = testNorm[features]\n",
    "ytest = testNorm['Diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_acc = np.zeros(len(lambda_val))\n",
    "index = 0\n",
    "\n",
    "#L2 regularization\n",
    "for l in lambda_val:\n",
    "    l2_acc[index] = runLRmodel(100, trainNorm, features, 'Diagnosis', 'l2', float(l))\n",
    "    index += 1\n",
    "    \n",
    "# penalty at which validation accuracy is maximum\n",
    "max_index_l2  = np.argmax(l2_acc)\n",
    "best_lambda = lambda_val[max_index_l2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best lambda value to refit the model and find top 5 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 33]\n",
      "Acc: [ 0.99  0.99  0.99  0.99  0.99  0.99  0.99  0.99  0.98  0.97]\n",
      "\n",
      "Best Lambda: 0.001\n",
      "\n",
      "Precision:0.833333333333\n",
      "Accuracy:0.9\n",
      "\n",
      "Top 5 features in decreasing order of feature weights:\n",
      "1  Weight, Feature('6.45', 'f23')\n",
      "2  Weight, Feature('3.93', 'f22')\n",
      "3  Weight, Feature('2.44', 'f21')\n",
      "4  Weight, Feature('2.42', 'f25')\n",
      "5  Weight, Feature('1.5', 'f4')\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  1]\n",
      " [ 1  5]]\n",
      "AUC:\n",
      "0.880952380952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Lambda: {}\".format(lambda_val)\n",
    "print \"Acc: {}\".format(l2_acc)\n",
    "print \n",
    "print \"Best Lambda: {}\".format(best_lambda)\n",
    "print \n",
    "l2 = LogisticRegression(C=1/best_lambda, penalty='l2')\n",
    "l2.fit(Xtrain, ytrain)\n",
    "pred_label = l2.predict(Xtest)\n",
    "true_label = ytest\n",
    "\n",
    "#model evaluation\n",
    "recall = recall_score(true_label, pred_label)\n",
    "precision = precision_score(true_label, pred_label)\n",
    "accuracy = accuracy_score(true_label, pred_label)\n",
    "f_score = f1_score(true_label, pred_label)\n",
    "\n",
    "#print 'Recall:{}'.format(recall)\n",
    "print 'Precision:{}'.format(precision)\n",
    "print 'Accuracy:{}'.format(accuracy)\n",
    "#print 'F1 score:{}'.format(f_score)\n",
    "print \n",
    "\n",
    "def top_5_feature(model):\n",
    "    coeff = abs(np.round(model.coef_[0,], decimals=2))\n",
    "    coeff_dict = dict()\n",
    "    index = 0\n",
    "    for i in features[0:30]:\n",
    "        coeff_dict[i] = str(list(coeff)[index])\n",
    "        index += 1\n",
    "    top_5 = sorted([(v,k) for k,v in coeff_dict.items()], reverse=True)[0:5]\n",
    "    rank = 1\n",
    "    for i in top_5:\n",
    "        print '{} '.format(str(rank)), 'Weight, Feature{}'.format(i)\n",
    "        rank += 1\n",
    "\n",
    "        \n",
    "\n",
    "print 'Top 5 features in decreasing order of feature weights:'\n",
    "top_5_feature(l1)\n",
    "print \n",
    "\n",
    "print 'Confusion matrix:'\n",
    "print metrics.confusion_matrix(true_label, pred_label)\n",
    "print 'AUC:'\n",
    "print roc_auc_score(true_label, pred_label)\n",
    "print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion for regularized model with L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization with lambda values of 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10 and 33 using L2 penalty did not show any significant drop or increase in model performance for all lambda values. But looking at the pattern, accuracy decreases with more regularization (higher lambda values).\n",
    "\n",
    "To see the effect of very small lambda value (less regularized model), let us use lambda = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\t\tScore\n",
      "Recall\t\t0.833333\n",
      "Precision\t0.714286\n",
      "Accuracy\t0.850000\n",
      "F1 score\t0.769231\n",
      "\n",
      "Confusion matrix:\n",
      "[[12  2]\n",
      " [ 1  5]]\n",
      "AUC:\n",
      "0.845238095238\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(C=1/0.0000001, penalty='l2')\n",
    "\n",
    "# fit model\n",
    "test_model = logistic.fit(Xtrain, ytrain)\n",
    "\n",
    "pred_label = test_model.predict(Xtest)\n",
    "true_label = ytest\n",
    "\n",
    "\n",
    "# metrics for test set\n",
    "test_recall = metrics.recall_score(true_label, pred_label)\n",
    "test_precision = metrics.precision_score(true_label, pred_label)\n",
    "test_accuracy = metrics.accuracy_score(true_label, pred_label)\n",
    "test_f_score = metrics.f1_score(true_label, pred_label)\n",
    "\n",
    "\n",
    "print \"Metrics\\t\\tScore\"\n",
    "print \"Recall\\t\\t{:f}\".format(test_recall)\n",
    "print \"Precision\\t{:f}\".format(test_precision)\n",
    "print \"Accuracy\\t{:f}\".format(test_accuracy)\n",
    "print \"F1 score\\t{:f}\".format(test_f_score)\n",
    "\n",
    "print \n",
    "print 'Confusion matrix:'\n",
    "print metrics.confusion_matrix(true_label, pred_label)\n",
    "print 'AUC:'\n",
    "print roc_auc_score(true_label, pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the performance report of the the logitic regression above, it can be seen that the area under ROC curve is less than the regularized ones. The model is underfitted and the more regularized models perform better.<br>\n",
    "\n",
    "Hence lambda values 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10 and 33 do not show underfitting or overfitting signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part 2: Handwritten digits recognition</u>\n",
    "#### 2.1 One-versus-rest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data set has 2520 datapoints with 785 columns which include label and features.\n",
      "The first column is the label, therefore the number of features in the data is 784.\n",
      "Unique labels in the data set is as follows:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#read and  analyse data set\n",
    "data_df = pd.read_csv(\"data/mnist/reduced_mnist.csv\")\n",
    "data = data_df.values\n",
    "\n",
    "print 'This data set has {} datapoints with {} columns which include label and features.'.format(data.shape[0],data.shape[1])  \n",
    "print 'The first column is the label, therefore the number of features in the data is {}.'.format(data.shape[1] - 1)\n",
    "print 'Unique labels in the data set is as follows:'\n",
    "print np.unique(data[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1764 data points.\n",
      "Test data: 756 data points.\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "train_data, test_data = train_test_split(data, test_size=0.3)\n",
    "print 'Training data: {} data points.'.format(train_data.shape[0])\n",
    "print 'Test data: {} data points.'.format(test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.874338624339\n",
      "Precision:0.874432343781\n",
      "Recall:0.874338624339\n"
     ]
    }
   ],
   "source": [
    "#train and assess model\n",
    "ncol_train = train_data.shape[1]\n",
    "train_X = train_data[0:,1:785]\n",
    "train_y = train_data[0:,:1]\n",
    "\n",
    "test_X = test_data[0:,1:785]\n",
    "test_y = test_data[0:,:1]\n",
    "\n",
    "ovr = OneVsRestClassifier(LogisticRegression(C=1/1, penalty='l1')).fit(train_X, train_y)\n",
    "pred_label = ovr.predict(test_X)\n",
    "true_label = test_y\n",
    "\n",
    "#model evaluation\n",
    "accuracy = metrics.accuracy_score(true_label, pred_label)\n",
    "precision = metrics.precision_score(true_label, pred_label, average = 'weighted')\n",
    "recall = metrics.recall_score(true_label, pred_label, average = 'weighted')\n",
    "\n",
    "print 'Accuracy:{}'.format(accuracy)\n",
    "print 'Precision:{}'.format(precision)\n",
    "print 'Recall:{}'.format(recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Choose the best value of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAverage\n",
      "Alpha\t\tTraining Acc.\tValidation Acc.\n",
      "0.100000\t0.837037\t0.814683\n",
      "1.000000\t0.834524\t0.818915\n",
      "3.000000\t0.837698\t0.813889\n",
      "10.000000\t0.832011\t0.818651\n",
      "33.000000\t0.839550\t0.818122\n",
      "100.000000\t0.851852\t0.816138\n",
      "333.000000\t0.864153\t0.823280\n",
      "1000.000000\t0.848677\t0.800132\n",
      "3333.000000\t0.810979\t0.699603\n",
      "10000.000000\t0.700132\t0.584921\n",
      "33333.000000\t0.540212\t0.078042\n"
     ]
    }
   ],
   "source": [
    "#alpha values\n",
    "alpha = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]\n",
    "\n",
    "#accuracy lists\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "#data\n",
    "train_data, test_data = train_test_split(data, test_size=0.3)\n",
    "train_X = train_data[0:,1:785]\n",
    "train_y = train_data[0:,:1]\n",
    "\n",
    "test_X = test_data[0:,1:785]\n",
    "test_y = test_data[0:,:1]\n",
    "\n",
    "print \"\\t\\t\\tAverage\"\n",
    "print \"Alpha\\t\\tTraining Acc.\\tValidation Acc.\"\n",
    "\n",
    "for a in range(len(alpha)):\n",
    "    int_train = []\n",
    "    int_val = []\n",
    "    for i in range(10):\n",
    "        Xtrain, Xval, ytrain, yval = train_test_split(train_X, train_y, test_size=0.3)\n",
    "        fit_train = OneVsRestClassifier(LogisticRegression(C=1./alpha[a], penalty='l1')).fit(Xtrain, ytrain)\n",
    "        fit_val = OneVsRestClassifier(LogisticRegression(C=1./alpha[a], penalty='l1')).fit(Xval, yval)\n",
    "        train_pred = fit_train.predict(test_X)\n",
    "        val_pred = fit_val.predict(test_X)\n",
    "        true_label = test_y\n",
    "        int_train.append(accuracy_score(true_label, train_pred))\n",
    "        int_val.append(accuracy_score(true_label, val_pred))\n",
    "    print \"{:f}\\t{:f}\\t{:f}\".format(alpha[a],np.array(int_train).mean(),np.array(int_val).mean())\n",
    "    train_accuracy.append(np.array(int_train).mean())\n",
    "    val_accuracy.append(np.array(int_val).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEsCAYAAAAsMK9uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNXZ9/HvDwRhAMVdWWaGoLLKOqK+rgRjUOOjGFwQ\nFzBKNNFEs6LGaEyISUwUTVREHzXKKC5xl7g90ShRwyKgAhJQdhQQBYGBCHK/f1R109PM0kDX1Ez1\n/bmuvqZr6Tp3dff0XXXOqVMyM5xzzjmARnEH4Jxzrv7wpOCccy7Nk4Jzzrk0TwrOOefSPCk455xL\n86TgnHMuzZOCcwklySQdmO91oyJpmKSJccbgPCk0aJJek/S5pF3jjsXFQ1IHSVsk3Rl3LC4ZPCk0\nUJJKgaMBA/4nojJ2iWK79ZWkxnHHsAPOBz4HzvKDA5cPnhQarvOBt4H7gQtSMyUdJumTzB84SYMk\nvRs+byRppKQPJa2S9KikPcNlpWE1wnckLQL+Ec5/LNzmGkmvS+qWse29JD0r6QtJkyX9JrMKQFJn\nSS9L+kzSHElnVrdDkoZLmi1praSPJH03a/mpkqaHZX0oaWA4f09J90laFp45PRXO36Y6IrOaRNL9\nku6UNEHSeqC/pJMlTQvLWCzp+qzXHyXpTUmrw+XDJB0qaXnWe366pBlV7GNtn08/SVPC8pdLurmG\n90sE34NfAJuAU2pY935JY8LPYq2kf0oqyVrteElzw327Pdw+kjpK+kf4fflUUrmk1tWUc6ekP2bN\ne1rSj8Lnqe/eWkmzJA2qZjup7+IuGfNek3RRxvSF4fflc0kvpvZHgVskrQjfx/ckda/uvXFZzMwf\nDfABzAO+B/Ql+EHYL2PZh8A3MqYfA0aGz39IkEzaAbsCdwEPh8tKCc48HgBaAM3D+RcCrcL1RwPT\nM7Y9PnwUAV2BxcDEcFmLcHo4sAvQG/gU6FrNPp0MdAQEHAtUAH3CZf2ANcA3CA5m2gKdw2XPA48A\newBNgGPD+cNSsWSUYcCB4fP7w20eGW6zGXAccEg43QNYDpwWrl8CrAWGhOXsBfQKl80CTswo50ng\nx9XsZ02fz1vAeeHzlsDhNXwHjgb+G+73n4Fna9nXtcAx4ed4a+Z7E677HNAaKAZWAgPDZQeG7/uu\nwD7A68DoamI6JvzMFU7vAWwA2oTTZwBtwvf3LGA9cED258XW7+IuGdt+DbgofH4qwf9AF4Lv1i+A\nN8Nl3wSmhvuicJ0D4v6fbSiP2APwxw58aHAUQSLYO5z+ALgyY/lvgHvD563Cf7yScHo2MCBj3QPC\nbe2S8Y/4tRrKbh2uszvQOHxtp6yyU//YZwFvZL3+LuC6HPfzKeCHGa+7pYp1DgC2AHtUsSz9I5Mx\nL/uH8oFaYhidKhe4CniymvV+DpSHz/ckSGhV/hDV8vm8Dvwq9dnWEts9wFPh8yPCz2LfGvZ1fMay\nlsBXQPuMdY/KWP4oYaKqotzTgGnVLBOwCDgmnL4Y+EcN+zAdODX786L2pPB34DsZyxqF73kJ8HXg\nP8DhQKOo/g+T+vDqo4bpAuAlM/s0nH6IjCqkcPp0BXXMpwPvmNnCcFkJ8GRYRbCaIEl8BeyX8frF\nqSeSGkv6XXjK/wWwIFy0N8FR4y6Z62c9LwEOS5UVljcU2L+qnZJ0oqS3w6qm1cBJYTkA7QmOsLO1\nBz4zs8+r2mYOMuNNVe+8KmmlpDXAJTnEADAOOEVSC+BMgmT4cTXr1vT5fAc4GPggrI77VlUbkNSc\n4Ki7HMDM3iL4MT4nl301s3XAZwRH7SmfZDyvIEgcSNpP0nhJS8PvwDi2vieVWPALPZ7gbIownvKM\nuM8PqwBT34fu1W2rFiXArRnb+YwgIbU1s38AfwFuB1ZIGitptx0ooyB5Umhgwh+DM4Fjw7rpT4Ar\ngZ6SegKY2SxgIXAiwT/lQxmbWExQzdE649HMzJZmrJM5dO45BKfqxxOcHZSmQiGoYthMUBWV0j6r\nrH9mldXSzC6tYr92Bf4G/JGgKqw1MCEsJ7WtjlW8JYuBPaup415PUK2VKqOqZJQ9TPBDwDMER9C7\nA2NyiIHw/XuL4Ef+PODBqtYL16328zGzuWY2BNgX+D3weJhosg0CdgPuyPgetKXywUG29GcjqSXB\nGc2yGtZP+S3B+3SIme0GnMvW96QqDwODwzr+wwg+V8Lpu4HLgL3Cz/j9ara1PvxblDEv8/NbDHw3\n67vV3MzeBDCz28ysL0GV5sHAT3PYT4cnhYboNIIj+65Ar/DRBXiDoNEx5SGC9oNjCOqsU8YAozIa\n5faRdGoN5bUiqLdeRfAP+tvUAjP7CngCuF5SkaTOWTE8Bxws6TxJTcLHoZK6VFFOU4I665XAZkkn\nAidkLP9fYLikAQoay9tK6hwejf+d4Mdxj7CMY8LXzAC6SeolqRlwfQ37mbm/n5nZRkn9qHzkXU7Q\nGHumpF0UNLL3ylj+APAzgjaJJ2opp8rPR9K5kvYxsy3A6nD2lipefwFwb1hW6ntwJMHBwSHVlHmS\ngobypsCvgbfNbHE162ZqBawD1khqSy0/sGY2jaDt6B7gRTNL7UcLguSyMtzX4QRnClVtYyWwFDg3\nPFu9kMoJeQxwlcJOD5J2l3RG+PzQ8IyvCUFy2UjV76GrgieFhucC4D4zW2Rmn6QeBKfLQzN6azxM\n0Fj7j4xqJggaGJ8BXpK0lqDR+bAaynuA4Kh2KUFj6ttZyy8jOIP4hODo+GGCJIKZrSX4YT+b4Ij0\nE4Kj3226Tobr/oCgLvtzgh/jZzKWTyJosL6FoHH4nwRVCBAcmW8iaFtZAVwRvuY/wA3AK8BcIJcL\no74H3BC+N78M40nFsIigSuvHBNUV04GeGa99MozpSTOrqKWc6j6fgcBMSesIPquzzWxD5gvDH+YB\nBI29n2Q8pgIvUP3ZwkPAdWHsfQmO+HPxK6APwfv+PLUnvFRZx1P5LGgW8CeCM6rlBAntXzVs42KC\nBLQK6Aa8mbGtJwm+S+PDKq33Cc68IDiDupvge7QwfP1NOcTs2NpDwLm8kPR7YH8zq6kaI7EkfUhQ\nrfFK3LFkknQ/sMTMfhF3LK5+8zMFt1MUXIfQI+wb3o+gofTJuOOKg6RvE1SP/CPuWJzbUZElBUn3\nhhePvF/Nckm6TdI8Se9K6hNVLC5SrQiqE9YTXCvwJ+DpWCOKgaTXgDuB74ftAc41SJFVH4WNfesI\n+oFv05gk6STgcoI62sOAW82sprpt55xzEYvsTMHMXido0KrOqQQJw8zsbaC1pAOiisc551zt4hzw\nrC2VLxxaEs7b5oIfSSOAEQAtWrTo27lz5zoJ0DnnkmLq1Kmfmtk+ta3XIEbBNLOxwFiAsrIymzJl\nSswROedcwyJpYe1rxdv7aCmVr35tF85zzjkXkziTwjPA+WEvpMOBNTWMFeOcc64ORFZ9JOlhgmGI\n95a0hOBKyiYAZjaGYFybkwiGv60guFrVOedcjCJLCuGgXjUtN+D7UZXvnHNu+/kVzc4559I8KTjn\nnEvzpOCccy7Nk4Jzzrk0TwrOOefSPCk455xL86TgnHMuzZOCc865NE8Kzjnn0jwpOOecS/Ok4Jxz\nLs2TgnPOuTRPCs4559I8KTjnnEvzpOCccy7Nk4Jzzrk0TwrOOefSPCk455xL86TgnHMuzZOCc865\nNE8Kzjnn0jwpONdQlZdDaSk0ahT8LS+POyKXALvEHYBzbgeUl8OIEVBREUwvXBhMAwwdGl9crsHz\nMwXnGqJrrtmaEFIqKoL5zu0ETwrO5UNdV+UsWgTAWUNu5KwhN24zP1JebZVoXn3k3M6KoyqnuBgW\nLuSmCaO3nR8lr7ZKPD9TcMkSx1FsHFU5o0ZBURHFa5ZTvGZ5MK+oKJgfJa+2SrzCSwpxnfoWWrlx\nSB3FLlwIZluPYuuoKifn+fkwdCiMHcvEficwsbQXlJTA2LHRH63Hsa+ubplZg3r07dvXdti4cWZF\nRWbBT0bwKCoK5kep0MpNlV1SYiYFf+uizJKSyvuaepSUJLNcMztzzJt25pg3Iy8nLcZ9dTsHmGI5\n/MbG/iO/vY+dSgqF9qMRV7lxJSOp6v2Voi03xuS79PMKW/p5ReTlpMV5oOF2iieFqsT1o1Fo5RZa\nEjSL5cwojpOxeAt2OyPXpFBYbQrV9cyIusdGcTHlDKGU+TTiK0qZTzlD6qTc7ZqfL4sWVb2/Udc7\njxpFeZNhlcttMiz6xlegnKGUsoBGbKGUBZQTbd1+qvlkeaMV7Fq6os6aT4Cg3WLBAtiyJfjrvY6S\nJZfMUZ8eO9umMK7JMCthvomvrIT5Nq7JsMiPdMZd+oYVsa7yGTfrbNylb0Rabmz7u9flVe/vXpdH\nW+44s6KmmyqX23RTIptuUidF+w150/Yb8qZX7bta4dVH24rrRyPWqv049nevtVXv715roy23JJ73\nOY5yUzWDjVpssEYtNtRZzWCsvNpqp+SaFBSs23CUlZXZlClTdui1paVBL8VsJSXBWXBUGjUK/mWz\nScEZeFQKbX8Lqdy4PtvYZF80B8F1GXXRDTchJE01s7La1ou0TUHSQElzJM2TNLKK5btLelbSDEkz\nJQ2PMp64uljHWLW/XfPzJcamm4IpN7x2jeYdl9O8Y3DxWl1cuwYxXfriF83VnVxOJ3bkATQGPgS+\nBjQFZgBds9a5Gvh9+Hwf4DOgaU3b3Znqo0LroVlo+1uI5ZYMD9oU6qo2JbYeqXH1pEsQ4m5TAI4A\nXsyYvgq4Kmudq4A7AAEdgHlAo5q2u9NtCgV0LVeh7W8hlrtq3X9t1br/1k1hFmOvX79obqfVh6Qw\nGLgnY/o84C9Z67QCXgU+BtYBJ1ezrRHAFGBKcXHxTr0xhdZWVWj766IV2wG7XzS303JNCpE1NEsa\nDAw0s4vC6fOAw8zssqx1jgR+BHQEXgZ6mtkX1W13ZxqanUuaF97/GICB3Q+ok/JibeAuLw/aEBYt\nChpsRo3yRubtUB8ampcC7TOm24XzMg0HnggT2TxgPtA5wpicS5T7/rWA+/61oM7KSzVwZ6qrBm6/\naK5uRHk/hcnAQZI6ECSDs4FzstZZBAwA3pC0H9AJ+CjCmJxLlLsvqPXAL69Sv8N+wJ5ckSUFM9ss\n6TLgRYKeSPea2UxJl4TLxwC/Bu6X9B5BY/PPzezTqGJyLml2a9akzsscOtSTQJJFeuc1M5sATMia\nNybj+TLghChjcC7Jnp2xDIBTeraJORKXFH47TucasHFvB62+nhRcvnhScK4Bu394v7hDcAnjScG5\nBqx508Zxh+ASprDup+Bcwjw5bQlPTlsSdxguQfxMwbkGbPykxQAM6t0u5khcUnhScK4BG3fRYXGH\n4BLGk4JzDViTxl4D7PLLv1HONWCPTVnMY1MWxx2GSxBPCs41YI9PXcLjU72h2eWPVx8514A98t0j\n4g7BJYyfKTjnnEvzpOBcA/bwpEU8PCnim267guJJwbkG7Ll3l/Hcu8viDsMliCcF5xqw8osOp/yi\nw+MOo06Ulwd3fmvUKPhbXp70guPhDc3OuXqvvBxGjICKimB64cJgGiK+t0NsBcen1ns0SzrEzN6r\no3hq5fdodm6rB99aAMB5R5TGGUbkYrs3dKw3pc6vfN6j+Q5JkyR9T9LueYjNOZcnr8xewSuzV8Qd\nRuQWVdOWXt38hl9wfGqtPjKzoyUdBFwITJU0CbjPzF6OPDrnXI3+emFh3E+huLjqA/bi4qQWHJ+c\nGprNbC7wC+DnwLHAbZI+kHR6lME55xzAqFFQVFR5XlFRMD+ZBcen1qQgqYekW4DZwNeBU8ysS/j8\nlojjc87V4N6J87l34vy4w4jc0KEwdmxQlS8Ff8eOrYO23tgKjk8uDc3/BO4BHjezDVnLzjOzByOM\nbxve0OzcVhf9dTIA91xwaMyRuPou14bmXLqkngxsMLOvwg03ApqZWUVdJwTnXGWeDFy+5dKm8ArQ\nPGO6KJznnHMuYXJJCs3MbF1qInxeVMP6zrk6Mvb1Dxn7+odxh+ESJJfqo/WS+pjZOwCS+gIbanmN\nc64OvLNwddwhuITJJSlcATwmaRkgYH/grEijcs7lZMx5feMOwSVMLhevTZbUGegUzppjZpuiDcs5\n51wcch0QrxPQFWgG9JGEmT0QXVjOuVzc8do8AL533IExR+KSotakIOk64DiCpDABOBGYCHhScC5m\ns5Z9EXcILmFyOVMYDPQEppnZcEn7AeOiDcs5l4u/nNMn7hBcwuTSJXWDmW0BNkvaDVgBtI82LOec\nc3HI5UxhiqTWwN3AVGAd8FakUTnncnLb/80F4AcDDoo5EpcUNSYFSQJuNLPVwBhJLwC7mdm7dRKd\nc65GH61cV/tKzm2HGpOCmZmkCcAh4fSCugjKOZeb0Wf3jjsElzC5tCm8I2mHRt2SNFDSHEnzJI2s\nZp3jJE2XNDMckdU551xMcmlTOAwYKmkhsJ7gqmYzsx41vUhSY+B24BvAEmCypGfMbFbGOq2BO4CB\nZrZI0r47uB/OFaSbX5oDwI9O6FTLms7lJpek8M0d3HY/YJ6ZfQQgaTxwKjArY51zgCfMbBGAmSX/\nZrPO5dGyNRvjDsElTC5Joea78FSvLbA4Y3oJwVlHpoOBJpJeA1oBt1Z1pbSkEcAIgOIE3xvVue31\nxzN6xh2CS5hc2hSeB54L//4f8BHw9zyVvwvQl+BGPt8ErpV0cPZKZjbWzMrMrGyfffbJU9HOOVe7\n8nIoLYVGjYK/5eVxRxStXAbEOyRzWlIf4Hs5bHsplS9yaxfOy7QEWGVm6wmG6H6d4Orp/+SwfecK\n3u9f+ACAnw/sHHMkyVReDiNGQEVFML1wYTANyb1Ncy5nCpWE91XIrgaqymTgIEkdJDUFzgaeyVrn\naeAoSbtIKgq3O3t7Y3KuUK2u+JLVFV/GHUZiXXPN1oSQUlERzE+qXAbE+1HGZCOgD7CstteZ2WZJ\nlwEvAo2Be81spqRLwuVjzGx2eEHcu8AW4B4ze38H9sO5gnTj6TV2AnQ7adGi7ZufBLk0NLfKeL6Z\noG3hb7ls3MwmEIysmjlvTNb0TcBNuWzPOefqUnFxUGVU1fykyqVN4Vd1EYhzbvuNej7o4X3NyV1j\njiSZRo2q3KYAUFQUzE+qWtsUJL0cXmSWmt5D0ovRhuWcy8XGTVvYuGlL3GEk1tChMHYslJSAFPwd\nOza5jcwAMqv5MgRJ082sV9a8aWYWy6ArZWVlNmXKlDiKds65BkvSVDMrq229XHoffSUpXYMmqYQd\nv6DNOedcPZZLUrgGmCjpQUnjgNeBq6INyzmXi189O5NfPTsz7jBcFGK6ai6XhuYXwgvWDg9nXWFm\nn0YblnPOFbAYr5rLpU1hEPAPM1sTTrcGjjOzpyKNrBrepuCcS7zS0qr7wpaUwIIFO7TJfLYpXJdK\nCADhXdiu26GonHPO1S7Gq+ZySQpVrZPLRW/OuYhd+9T7XPuUDwKQONVdHVcHV83lkhSmSLpZUsfw\ncQswNerAnHO1a9akEc2abPcQZq6+GzUquEouUx1dNZfLEf/lwLXAI+H0y+Q2SqpzLmJ+JXNCpRqT\nr7kmqDIqLg4SQh1cNVdrQ/M2L5CaAaeY2WPRhFQzb2h2zrntl8+GZiQ1lnSSpAeBBcBZOxmfcy4P\nrnriXa564t24w3AJUmP1kaRjCe6jfBIwCTgS+JqZVdT0Oudc3Whd1DTuEFzCVJsUJC0BFgF3Aj8x\ns7WS5ntCcK7+8DuuuXyrqfrocaANQVXRKZJa4GMeOedcolWbFMzsCqAD8CfgOGAOsI+kMyW1rJvw\nnHM1+cljM/jJYzPiDsMlSI1tChZ0TXoVeFVSE+CbwBDgDmDv6MNzztWkze7N4g7BJUzOVyab2Sbg\nOeA5Sc2jC8k5l6sfndAp7hBcwuzQpZBmtiHfgTjnnIufXx/vXAN2xfhpXDF+WtxhuATJufpIUpF3\nR3WufvnaPt7nw+VXrUlB0v8D7gFaAsWSegLfNTMf/8i5mP1gwEFxh+ASJpfqo1sIeh2tAjCzGcAx\nUQblnHMuHjm1KZjZ4qxZX0UQi3NuO1320Dtc9tA7cYfhEiSXNoXFYRWShdcq/BCYHW1YzrlcdG2z\nW9whuITJJSlcAtwKtAWWAi8B348yKOdcbr533IFxh+ASptakYGafAtHf2cE551zscul9dFsVs9cA\nU8zs6fyH5JzL1SUPBnfGHXNe35gjcUmRS/VRM6AzkLrT2reB+UBPSf3DgfOcczHoU9I67hBcwuSS\nFHoAR5rZVwCS7gTeAI4C3oswNudcLUYc0zHuEFzC5NIldQ+CC9dSWgB7hkniv5FE5ZxzLha5nCn8\nAZgu6TVABBeu/Ta86c4rEcbmnKvFRX+dDMA9FxwacyQuKXLpffS/kiYA/cJZV5vZsvD5TyOLzDlX\nq//X0W9r4vIr1wHxNgIfEzQ6HyjpQDN7PbqwnHO5uPCoDnGH4BKm1jYFSRcBrwMvAr8K/16fy8Yl\nDZQ0R9I8SSNrWO9QSZslDc4tbOecc1HIpaH5h8ChwEIz6w/0BlbX9iJJjYHbgROBrsAQSV2rWe/3\nBFdKO+e2wwX3TuKCeyfFHYZLkFyqjzaa2UZJSNrVzD6QlMs9APsB88zsIwBJ44FTgVlZ610O/I0g\n8TjntsPxXfaNOwSXMLkkhSWSWgNPAS9L+hxYmMPr2gKZo6suAQ7LXEFSW2AQ0J8akoKkEcAIgOLi\n4hyKdq4wnHdEadwhuITJpffRoPDp9ZJeBXYHXshT+aOBn5vZFkk1xTAWGAtQVlZmeSrbOedclhqT\nQljfP9PMOgOY2T+3Y9tLgfYZ0+3CeZnKgPFhQtgbOEnSZjN7ajvKca5gDb3nbQDKLzo85khcUtSY\nFMzsq7D3ULGZLdrObU8GDpLUgSAZnA2ck7X9dH86SfcDz3lCcC533+rRJu4QXMLk0qawBzBT0iRg\nfWqmmf1PTS8ys82SLiPowtoYuNfMZkq6JFw+ZsfDds4BDOnnbWwuv3JJCtfu6MbNbAIwIWtelcnA\nzIbtaDnOOefyI5eG5n9KKgEOMrNXJBURHPk752J21l1vAfDId4+IORKXFLncZOdigu6gewIdCbqa\njgEGRBuac642g/u2izsElzC5VB99n+BCtH8DmNlcSX7FjHP1wBll7WtfybntkMswF/81sy9TE5J2\nAfxaAefqgU1fbWHTV1viDsMlSC5J4Z+SrgaaS/oGwW05n402LOdcLs6959+ce8+/4w7DJUgu1Ucj\nge8Q3HrzuwS9ie6JMijnXG7O7ufVRy6/ckkKpwEPmNndUQfjnNs+g3p7Q7PLr1yqj04B/iPpQUnf\nCtsUnHP1wIYvv2LDl1/FHYZLkFqTgpkNBw4kaEsYAnwoyauPnKsHht03iWH3+f0UXP7kdNRvZpsk\n/Z2g11Fzgiqli6IMzDlXu3MPL4k7BJcwuVy8diJwFnAc8BpBI/OZkUblnMvJKT19QDyXX7mcKZwP\nPAJ818z+G3E8zrnt8MXGTQDs1qxJzJG4pMhl7KMhmdOSjgKGmNn3I4vKOZeTi/86BfCxj1z+5NSm\nIKk3wb0QzgDmA09EGZRzLjfDjyyNOwSXMNUmBUkHE/Q2GgJ8SlCFJDPrX0exOedqMbD7AXGH4BKm\npjOFD4A3gG+Z2TwASVfWSVTOuZx8tj4YlmzPFk1jjsQlRU3XKZwOfAy8KuluSQMA1U1YzrlcXDpu\nKpeOmxp3GC5Bqj1TCO+V/JSkFsCpwBXAvpLuBJ40s5fqKEbnXDUuPvprcYfgEiaX3kfrgYeAhyTt\nQdDY/HPAk4JzMTu+635xh+ASJpexj9LM7HMzG2tmftc15+qBFWs3smLtxrjDcAnig9s514Bd/tA0\nwK9TcPnjScG5BuzS4zrGHYJLGE8KzjVgx3Xy26W7/NquNgXnXP2ybPUGlq3eEHcYLkH8TMG5BuzK\nR6YD3qbg8seTgnMN2OVfPyjuEFzCeFJwrgE76qC94w7BJYy3KTjXgC1aVcGiVRVxh+ESxM8UnGvA\nfvr4DMDbFFz+eFJwrgG78hsHxx2CSxhPCs41YId/ba+4Q3AJ420KzjVgH65cx4cr18UdhksQP1Nw\nrgG7+on3AG9TcPnjScG5BuxnAzvFHYJLmEirjyQNlDRH0jxJI6tYPlTSu5Lek/SmpJ5RxuNc0vQt\n2ZO+JXvGHYZLkMiSgqTGwO3AiUBXYIikrlmrzQeONbNDgF8DY6OKx7kkmvPJWuZ8sjbuMFyCRHmm\n0A+YZ2YfmdmXwHiC23qmmdmbZvZ5OPk20C7CeJxLnF8+/T6/fPr9uMNwCRJlm0JbYHHG9BLgsBrW\n/w7w96oWSBoBjAAoLi7OV3zONXhXn9Ql7hBcwtSLhmZJ/QmSwlFVLTezsYRVS2VlZVaHoTlXr/Vs\n3zruEFzCRJkUlgLtM6bbhfMqkdQDuAc40cxWRRiPc4kzc9kaALq12T3mSFxSRNmmMBk4SFIHSU2B\ns4FnMleQVAw8AZxnZv+JMBbnEumGZ2dxw7Oz4g7DJUhkZwpmtlnSZcCLQGPgXjObKemScPkY4JfA\nXsAdkgA2m1lZVDE5lzS/PCW7Q59zO0dmDauKvqyszKZMmRJ3GM4516BImprLQXe9aGjeWZs2bWLJ\nkiVs3Lgx7lASpVmzZrRr144mTZrEHYqrxozFqwFvcHb5k4iksGTJElq1akVpaSlhNZTbSWbGqlWr\nWLJkCR06dIg7HFeN306YDfjYRy5/EpEUNm7c6AkhzySx1157sXLlyrhDcTW44dTucYfgEiYRSQHw\nhBABf0/rv077t4o7BJcwfj8F5xqwqQs/Y+rCz+IOwyWIJ4U8WLVqFb169aJXr17sv//+tG3bNj39\n5Zdf5rSN4cOHM2fOnBrXuf322ykvL89HyC4h/vDCHP7wQs3fG+e2RyK6pM6ePZsuXbZjDJjycrjm\nGli0CIqLYdQoGDo0L/Fdf/31tGzZkp/85CeV5psZZkajRg0rD2/3e+vqVOquax33aRlzJK6+y7VL\nasP6hcqH8nIYMQIWLgSz4O+IEcH8PJs3bx5du3Zl6NChdOvWjY8//pgRI0ZQVlZGt27duOGGG9Lr\nHnXUUUxuD5anAAASx0lEQVSfPp3NmzfTunVrRo4cSc+ePTniiCNYsWIFAL/4xS8YPXp0ev2RI0fS\nr18/OnXqxJtvvgnA+vXr+fa3v03Xrl0ZPHgwZWVlTJ8+Pe/75uqHjvu09ITg8qrwksI110BFReV5\nFRXB/Ah88MEHXHnllcyaNYu2bdvyu9/9jilTpjBjxgxefvllZs3adoiCNWvWcOyxxzJjxgyOOOII\n7r333iq3bWZMmjSJm266KZ1g/vznP7P//vsza9Ysrr32WqZNmxbJfrn64e2PVvH2Rz5kmMufwksK\nixZt3/yd1LFjR8rKtp6xPfzww/Tp04c+ffowe/bsKpNC8+bNOfHEEwHo27cvCxYsqHLbp59++jbr\nTJw4kbPPPhuAnj170q1btzzujatvbnn5P9zysg8b5vInMV1Sc1ZcHFQZVTU/Ai1atEg/nzt3Lrfe\neiuTJk2idevWnHvuuVVehd20adP088aNG7N58+Yqt73rrrvWuo5LtpsG+x1sXX4V3pnCqFFQVFR5\nXlFRMD9iX3zxBa1atWK33Xbj448/5sUXX8x7GUceeSSPPvooAO+9916VZyIuOYr3KqJ4r6LaV3Qu\nR4V3ppDqZRRR76Oa9OnTh65du9K5c2dKSko48sgj817G5Zdfzvnnn0/Xrl3Tj91397H2k2ri3E8B\nOOqgvWOOxCVFYXZJTbDNmzezefNmmjVrxty5cznhhBOYO3cuu+yyY/nf39v67ay73gJ87CNXu4Ia\nJdVttW7dOgYMGMDmzZsxM+66664dTgiu/rvlrF5xh+ASxn8tEqZ169ZMnTo17jBcHWnTunncIbiE\nKbyGZucS5LU5K3htzoq4w3AJ4mcKzjVgd772IQDHddo35khcUviZgnMN2J/P6c2fz+kddxguAuXl\nUFoKjRoFf+tqLEw/U3CuAdu3VbO4Q3ARSA3RlhqRJzVEG0Tfe97PFPKgf//+21yINnr0aC699NJq\nX9OyZTCI2bJlyxg8eHCV6xx33HFkd7/NNnr0aCoyxnI66aSTWL16da6huwbulVnLeWXW8rjDcHlW\nx0O0VVKQSSHfp2VDhgxh/PjxleaNHz+eIUOG1PraNm3a8Pjjj+9w2dlJYcKECbRu7TdxLxR3v/ER\nd7/xUdxhuDyr4yHaKim4pBDFyNmDBw/m+eefT99QZ8GCBSxbtozevXszYMAA+vTpwyGHHMLTTz+9\nzWsXLFhA9+7BfXY3bNjA2WefTZcuXRg0aBAbNmxIr3fppZemh9y+7rrrALjttttYtmwZ/fv3p3//\n/gCUlpby6afBVa4333wz3bt3p3v37ukhtxcsWECXLl24+OKL6datGyeccEKlclzDcue5fbnz3L5x\nh+HyrLqh2CIaoq2y1M1fGsqjb9++lm3WrFnbzKtOSYlZkA4qP0pKct5ElU4++WR76qmnzMzsxhtv\ntB//+Me2adMmW7NmjZmZrVy50jp27GhbtmwxM7MWLVqYmdn8+fOtW7duZmb2pz/9yYYPH25mZjNm\nzLDGjRvb5MmTzcxs1apVZma2efNmO/bYY23GjBnh/pTYypUrM/YvmJ4yZYp1797d1q1bZ2vXrrWu\nXbvaO++8Y/Pnz7fGjRvbtGnTzMzsjDPOsAcffLDa/dqe99Y5lx/jxpkVFVX+jSoqCubvKGCK5fAb\nW3BnClGdlmVWIaWqjsyMq6++mh49enD88cezdOlSli+vvv739ddf59xzzwWgR48e9OjRI73s0Ucf\npU+fPvTu3ZuZM2fWOtDdxIkTGTRoEC1atKBly5acfvrpvPHGGwB06NCBXr2CK2FrGprb1X8vvP8x\nL7z/cdxhuDwbOhTGjoWSEpCCv2PH1skQbYXX+yiqkbNPPfVUrrzySt555x0qKiro27cv999/PytX\nrmTq1Kk0adKE0tLSKofKrs38+fP54x//yOTJk9ljjz0YNmzYDm0nJTXkNgTDbnv1UcN1378WADCw\n+wHxBuLybujQukkC2QruTCGqkbNbtmxJ//79ufDCC9MNzGvWrGHfffelSZMmvPrqqyysKhtlOOaY\nY3jooYcAeP/993n33XeBYMjtFi1asPvuu7N8+XL+/ve/p1/TqlUr1q5du822jj76aJ566ikqKipY\nv349Tz75JEcfffTO7aSrd+6+oIy7L6h1jDPnclZwZwpRjpw9ZMgQBg0alK5GGjp0KKeccgqHHHII\nZWVldO7cucbXX3rppQwfPpwuXbrQpUsX+vYNGhB79uxJ79696dy5M+3bt6805PaIESMYOHAgbdq0\n4dVXX03P79OnD8OGDaNfv34AXHTRRfTu3durihJmt2ZN4g7BJYwPne1q5O9t/fbsjGUAnNKzTcyR\nuPrOh852rgCMezuokvSk4PLFk4JzDdj9w/vFHYJLmMQkBTNDUtxhJEpDq1osRM2bNo47BJcwieh9\n1KxZM1atWuU/YnlkZqxatYpmzXzAtfrsyWlLeHLakrjDcAmSiDOFdu3asWTJElauXBl3KInSrFkz\n2rVrF3cYrgbjJy0GYFBv/5xcfiQiKTRp0oQOHTrEHYZzdW7cRYfFHYJLmEirjyQNlDRH0jxJI6tY\nLkm3hcvfldQnynicS5omjRvRpHEiaoFdPRHZt0lSY+B24ESgKzBEUtes1U4EDgofI4A7o4rHuSR6\nbMpiHpuyOO4wXIJEeYjRD5hnZh+Z2ZfAeODUrHVOBR4IB/F7G2gtyQdxcS5Hj09dwuNTvaHZ5U+U\nbQptgcxDmCVAdgVoVeu0BSoN+yhpBMGZBMA6SXPyEN/ewKd52I6X6+XGXe7ej15SMPvq5e64klxW\nahANzWY2Fhibz21KmpLLJd/55uV6uUko08tNbrlRVh8tBdpnTLcL523vOs455+pIlElhMnCQpA6S\nmgJnA89krfMMcH7YC+lwYI2Z+R1DnHMuJpFVH5nZZkmXAS8CjYF7zWympEvC5WOACcBJwDygAhge\nVTxVyGt1lJfr5cZYbiHtq5cbsQY3dLZzzrno+FUvzjnn0jwpOOecS/Ok4JxzLq1BXKfgtp+kfoCZ\n2eRweJGBwAdmNiHm0Jxz9VjBnylIahlTuZH1tJJ0HXAbcKekG4G/AC2AkZKuiarcOEjaX9Kdkm6X\ntJek6yW9J+nRpA6ZIqmRpEbh86aS+kjaM+64ohB2Vz9T0hnh8wHhIJrfS70HLr8KvveRpEVmVpyk\nciW9B/QCdgU+AdqZ2ReSmgP/NrMeEZW7O3AVcBqwL2DACuBp4HdmtjqCMl8AnidIeucA5cBDYQzH\nm1n2eFv5Ll8E43y1DWctBSZZRP9Ykk4D7gK2AJcAVwPrgE7ApWb2bBTlhmV/k+B9zdzXp83shQjL\nvIPgu9QU+ILgO/0McDKw3Mx+GFXZGTHsCWBmn9VBWZ2BWwg+3x8A1xK85/8BLjCz2ZHHUAhJQdKP\nqlsEXGNmkRxlSXq3hnIPNrNdIyp3mpn1zn4eTk83s14Rlfsi8A/gr2b2SThvf+ACYICZnRBBmZn7\nWinRRrmv4fZPAO4A5rL1Svx2wIHA98zspQjKnEYwunBzYAZwqJnNkVQC/C2q4RAkjQYOBh4gGKMM\ngn09H5gb1Y+zpPfM7BBJTQgOcA4wsy8l7QK8E+EBTjHwB2AAsJrgf3Y3gu/3SDNbEFG5rwM3AS2B\n3wE/Bx4BvgVcYWYDoig3U6G0KfyW4I3eXMWyKE9B9wO+CXyeNV/AmxGW+6WkIjOrAPqmCw2O5LdE\nWG6pmf0+c0aYHH4v6cKIysz8/B6oYVkUbiU4G1mQOVNSB4ILM7tEUWhGwl1kZnPCeQsjrk45ycwO\nzp4p6RGCo9iojtg3A5jZJkmTwxGXUxfHRvldfgQYDQw1s68gfTuAMwhGfD48onJbpc72JP3azMaH\n85+V9KuIyqykUJLCO8BTZjY1e4GkiyIs9zmgpZlNr6Lc1yIs9xgz+y+AmWX+4zQhOGqPykJJPyM4\nU1gOIGk/YBiVR8PNp6cltTSzdWb2i9RMSQcS/FhFaRe2HjVnWkrwXkdCUqPwc70wY15jgiqWqGyU\ndKiZTc6afyiwMcJyP8n4fAemZoZnoF9GWO7eZvZI5owwOYyX9OsIy22c8fzmrGVRfr5phVJ91AlY\nZWbbDD8rab/UD5jbOZL2AEYS3Cdj33D2coI64N+ZWfYZU77KjaWnlaSrgDMJjhxTSa89wThfj5rZ\njRGUeSjwnpltzJpfChxlZuPyXWa4/T4EN8FqxdZE2B5YA3y/qgOuKElqAbQwsxURbX888BnwVyp/\nthcQJIwzIyr3u0C5ma3Lmn8gcJmZXRFFuZXKKoSk4OInabiZ3RfBdq8jqGPfBXiZ4J4drwLfAF40\ns1H5LjOr/K7A/1C58fUZM5sVZblxCY/Q0/uaqsqKuMxGEJz1hoNrdgcWRNnwG5bzHYIDnEqfLfC/\nqTPxJCr4pCBpRHi/BhehqHpbxdXTKi7hj/J1BG1DvwQuB74NzAZ+GOUow3Xd0yosM7beVnEI3+Mz\nCHruPQ58nSAxfQCMyaoOjkShtCnURHEHkBS19LbaL6JiN4d1vRWSPjSzLwDMbEPEDZGxdMEF7mdr\nF9xXCbrgnhTGMIZtb3mbFzX1tJIUSU+r0HVAT6rpbQUkqgsuwX3tU11wT6VyF9xORNegn1YwZwph\n/99tTgXrot9voZC0nBp6W5lZmwjK/DfQ38wqMhpgUz/Yr5pZn3yXmVF2wXTBlTQbOLG6nlZmFklP\nq6z9fd/Mumcseyeqz7fQuuBmKogzBUk/B4YQNAhOCme3Ax6WNN7MfhdbcMkSR2+ruHpaQWF1wY2l\npxXE1tuq0LrgphVEUiBoMOpmZpsyZ0q6GZhJcJGI20lm9p0alp0TUZlVNviFPc2ivsl6IXXBvReY\nHPbKye5p9b8RljuC4Md/o5lNypjfnmj/bwutC25aQVQfSfoA+KaZLcyaXwK8ZGad4onMNWQF2AW3\nYHpaFVoX3EplFUhSGEgwKNxcth7lFBMMR3BZxA1HrgAltQtuXYuzt1VG+Ynvglup/EJICpB+o7O7\n001OXcLuXD4lrQtuTD2tYh3wsFC74BZMUnAu32rpghvJgIcqoMEOwzLi6m1V54MdhuXGMuBhpkJp\naHYuCnEMeFhIgx1CfL2tYhnsEGIb8DDNk4JzO66QuuDG0dMK4uttVWhdcLeW79VHzrnaxNXTKiy7\nzntbxTHYYVhuLAMeVirLk4JzbmdE1dMq3HZsva0KqQtuJk8KzrmdElVPq3DbPuBhHXbBBW9TcM7l\nIKbBDiGmAQ/j6oJLTAMeZvIzBedcreIY7DAsN5YBDwutC24mP1NwzuWi0G4tW2hdcNM8KTjnahXH\nYIfhtuMa8LDQuuCmefWRc85lKbQuuJXK96TgnHO5S2oX3HQMnhSccy53Se+C620KzjmXpdC64Gby\npOCcc9uKY7BDiG/AwzRPCs45t61C64Kb5m0Kzjnn0urkYgjnnHMNgycF55xzaZ4UXOJIOk2SSeqc\nMa9U0vu1vK7WdfJJ0jBJf6mr8pzLhScFl0RDgInhX+fcdvCk4BJFUkvgKOA7BHfJqmqdYZKelvSa\npLnhVaQpjSXdLWmmpJfCi4aQdLGkyZJmSPqbpKKsbTaStEBS64x5cyXtJ+kUSf+WNE3SK+EYOtkx\n3S9pcMb0uoznPw3LflfSr8J5LSQ9H8bzvqSzduwdc64yTwouaU4FXjCz/wCrJPWtZr1+BDcv6QGc\nIaksnH8QcLuZdQNWh+sAPGFmh5pZT4IbnlQaIC7sPvg0MAhA0mHAwnAwtYnA4eGQyOOBn+W6M5JO\nCGPqR3Cla19JxxCMh7PMzHqaWXfghVy36VxNPCm4pBlC8MNL+Le6KqSXzWyVmW0AniA4uwCYn9E3\nfSpQGj7vLumNcBiCoUC3Krb5CJA6Yj87nAZoB7wYvvan1by2OieEj2nAO0BngiTxHvANSb+XdLSZ\nrdmObTpXLb94zSWGpD2BrwOHSDKgMWCSflrF6tkX6KSmM4dq/gpoHj6/HzjNzGZIGgYcV8U23wIO\nlLQPwZ2yfhPO/zNws5k9I+k44PoqXruZ8CBNUiOgaWq3gBvN7K7sF0jqQ3BXrt9I+j8zu6GK7Tq3\nXfxMwSXJYOBBMysxs1Izaw/MB46uYt1vSNozbDM4DfhXLdtuBXwsqQnBmcI2LLgS9EngZmC2ma0K\nF+1OcNN3qP6q1AVsHdbgfwiuYAV4EbgwbCtBUltJ+0pqA1SY2TjgJiCSO5C5wuNJwSXJEIIf5Ux/\no+oqpEnhsneBv5nZlFq2fS3wb4Lk8UEN6z0CnMvWqiMIzgwekzSV6m8MczdwrKQZwBHAegAzewl4\nCHgrrH56nCBBHQJMkjSd4Ebvv6lyq85tJx/mwhWcsPqnzMwuizsW5+obP1NwzjmX5mcKzjnn0vxM\nwTnnXJonBeecc2meFJxzzqV5UnDOOZfmScE551za/wdlrn0pxsEUuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e2d9b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Alpha: 333\n"
     ]
    }
   ],
   "source": [
    "#plot graph\n",
    "max_index_1  = np.argmax(train_accuracy)\n",
    "max_index_2  = np.argmax(val_accuracy)\n",
    "best_alpha = alpha[max_index_2]\n",
    "\n",
    "plt.scatter(range(0,len(alpha)), train_accuracy, color='r', label='Training')\n",
    "plt.scatter(range(0,len(alpha)), val_accuracy, color='b', label='Validation')\n",
    "plt.xticks(range(0,len(alpha)), alpha, rotation='vertical') \n",
    "#plt.plot((np.argmax(train_accuracy), np.argmax(train_accuracy)), (0, val_accuracy[max_index_1]), ls='dotted')\n",
    "plt.plot((np.argmax(val_accuracy), np.argmax(val_accuracy)), (0, train_accuracy[max_index_2]), ls='dotted')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 1.0])\n",
    "plt.xlabel('Alpha values')    \n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Average accuracy vs Alpha values')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "print \n",
    "print \"Best Alpha: {}\".format(best_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it can be seen that validation accuracy is almost the same or as good as training accuracy at alpha values 0.1, 1, 3, 10, 33 and 100. Whereas from alpha = 1000 to 33333, the validation performance is decreasing and produces gap between validation and training accuracy. This is a sign of high bias and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original total no. of features: 784\n",
      "The new no. of features (non-zero features): 273\n",
      "\n",
      "Confusion matrix of test set:\n",
      "[[62  0  0  1  0  0  0  0  1  0]\n",
      " [ 0 88  0  0  0  0  0  0  1  0]\n",
      " [ 0  1 67  0  0  0  2  2  5  2]\n",
      " [ 0  0  3 70  0  4  1  1  3  2]\n",
      " [ 1  0  0  1 68  0  3  1  1  1]\n",
      " [ 3  1  2  4  0 52  3  1  4  2]\n",
      " [ 2  0  4  0  0  0 76  0  0  0]\n",
      " [ 1  1  2  0  1  0  0 65  1  7]\n",
      " [ 1  3  0  2  2  2  1  1 59  2]\n",
      " [ 0  0  0  0  7  1  1  1  1 48]]\n"
     ]
    }
   ],
   "source": [
    "print \"The original total no. of features: {}\".format(train_X.shape[1])\n",
    "\n",
    "#getting features\n",
    "ovr = OneVsRestClassifier(LogisticRegression(C=1./best_alpha, penalty='l1')).fit(train_X, train_y)\n",
    "ovr.coef_\n",
    "model = SelectFromModel(ovr, prefit=True)\n",
    "\n",
    "#non-zero features\n",
    "train_X_new = model.transform(train_X)\n",
    "test_X_new = model.transform(test_X)\n",
    "print \"The new no. of features (non-zero features): {}\".format(train_X_new.shape[1])\n",
    "print \n",
    "\n",
    "#train final model\n",
    "final_model = OneVsRestClassifier(LogisticRegression(C=1./best_alpha, penalty='l1')).fit(train_X_new, train_y)\n",
    "pred_label = final_model.predict(test_X_new)\n",
    "true_label = test_y\n",
    "\n",
    "cm = metrics.confusion_matrix(true_label, pred_label, labels = np.unique(data[:,0]))\n",
    "print 'Confusion matrix of test set:'\n",
    "print cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93        64\n",
      "          1       0.94      0.99      0.96        89\n",
      "          2       0.86      0.85      0.85        79\n",
      "          3       0.90      0.83      0.86        84\n",
      "          4       0.87      0.89      0.88        76\n",
      "          5       0.88      0.72      0.79        72\n",
      "          6       0.87      0.93      0.90        82\n",
      "          7       0.90      0.83      0.87        78\n",
      "          8       0.78      0.81      0.79        73\n",
      "          9       0.75      0.81      0.78        59\n",
      "\n",
      "avg / total       0.87      0.87      0.87       756\n",
      "\n",
      "Accuracy\n",
      "0.97\n",
      "0.99\n",
      "0.85\n",
      "0.83\n",
      "0.89\n",
      "0.72\n",
      "0.93\n",
      "0.83\n",
      "0.81\n",
      "0.81\n"
     ]
    }
   ],
   "source": [
    "classes = ['0', '1', '2', '3', '4', '5','6', '7', '8', '9']\n",
    "print(classification_report(true_label, pred_label, target_names=classes))\n",
    "\n",
    "tptn = cm.diagonal()\n",
    "tpfpfntn = cm.sum(axis=1)\n",
    "a = []\n",
    "b= []\n",
    "print \"Accuracy\"\n",
    "for i in range(len(classes)):\n",
    "    a.append(float(tptn[i]))\n",
    "    b.append(float(tpfpfntn[i]))\n",
    "    print round(a[i]/b[i], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lambda value of 333, the average accuracy value of test set is 0.87 while average accuracy of training set is 0.864. Since the test set has better performance, this model is not overfitted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
